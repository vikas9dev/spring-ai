## Unit Testing Concepts Demo with Spring AI

This blog post details a demonstration of unit testing concepts using a basic Spring AI project.

The project structure includes:

*   A controller named `ChartController`.
*   Two APIs: `/chart` and `/prompt-stuffing`.

### `/chart` API

The `/chart` API simply passes the received prompt to the Language Model (LM) using a chat client bean. This bean is created in the constructor of the `ChartController` class. Invoking this API allows users to ask any question to the LM model.

### `/prompt-stuffing` API

The `/prompt-stuffing` API provides a system message by loading data from the `Air Policy` template located in the resources folder. This API simulates a Retrieval-Augmented Generation (RAG) scenario, providing the LM model with context to answer questions. The `Air Policy` template contains air policy-related data.

The goal is to write unit tests for both of these REST APIs.

### Application Properties

The `application.properties` file contains the following configurations:

*   Logging pattern for the console.
*   Log level set to debug for advisor-related packages.
*   OpenAI API key set using an environment variable. The value of the environment variable is assigned to the `openai.api-key` property, which is used by the Spring AI framework to interact with the OpenAI LM model.

### Dependencies (pom.xml)

The `pom.xml` file includes the following dependencies:

*   `spring-boot-starter-web`
*   `spring-ai-openai`
*   `spring-boot-devtools`
*   `spring-boot-starter-test` (the primary dependency for unit testing)

The project is available in a GitHub repository.

### Test Package

Under the standard Spring Boot project structure, the test files are located in the `src/test/java` directory, mirroring the main application's package structure (e.g., `com.easy.springai`). The default generated test file will be enhanced to include unit testing code.

### Spring AI Evaluator Component

The Spring AI framework includes an interface called `Evaluator`, which is crucial for unit testing AI scenarios.

#### Evaluator Interface

The `Evaluator` interface is a functional interface with a single abstract method: `evaluate`.

```java
EvaluationResponse evaluate(EvaluationRequest request);
```

*   `EvaluationRequest`: Contains the user-provided prompt text, the LM model's response content, and optional RAG-related data in a list. If there is no RAG or prompt stuffing scenario, the data list can be empty.
*   `EvaluationResponse`: Contains details about the evaluation, such as whether the answer passed, the score, feedback, and metadata.

The `Evaluator` interface also has a default method that converts the context data (from the data list) into a single string, separated by line separators. This utility method is useful for consolidating multiple document data into a single string.

#### Evaluator Implementations

The `Evaluator` interface defines a contract for performing unit testing of GenAI scenarios. There are a couple of implementations:

1.  **Relevancy Evaluator**: Checks if the response generated by the LM model is relevant to the question.
2.  **Fact Checking Evaluator**: Checks if the answer provided by the LM model is correct.

##### Relevancy Evaluator

The `RelevancyEvaluator` determines if the LM model's response aligns with the provided context.

*   It uses a prompt template with instructions for the LM model:
    > "Your task is to evaluate if the response for the query is in line with the context information provided. You have two options to answer either yes or no. Answer yes if the response for the query is in line with the context information, otherwise no."
*   The template includes placeholders for the query (original prompt), the LM model's response, and any context information (for RAG scenarios).
*   The framework makes a call to the LM model, providing the user question, the response, and the context information.
*   Based on the LM model's evaluation (yes or no), the `passing` variable is set to `true` (and the score to 1) if relevant, or `false` (and the score to 0) if not.
*   The `EvaluationResponse` object is then constructed with this information.

##### Fact Checking Evaluator

The `FactCheckingEvaluator` verifies the correctness of the LM model's answer. Relevancy alone is not sufficient; the answer must also be factually accurate.

üìå **Example:** If the question is "What is the capital of India?" and the LM model responds with "India is a great country with various cultures and languages," the answer is relevant to India but does not provide the correct answer.

*   It uses a prompt template with instructions for the LM model:
    > "Evaluate whether or not the following claim is supported by the provided document. Respond with yes if the claim is supported, or no if it is not."
*   The framework asks the LM model to answer "yes" if the claim is supported by the document, and "no" if it is not.
*   The `evaluate` method passes the relevant information to the LM model.
*   The `passing` Boolean variable in the `EvaluationResponse` object indicates whether the answer is correct.
*   The framework does not provide a score for fact-checking, only a pass/fail indication.

üìù **Note:** Currently, the relevancy score is either 0 or 1. Future versions of the framework may provide more granular relevancy scores (e.g., 0.5, 0.7) to indicate partial relevancy.
