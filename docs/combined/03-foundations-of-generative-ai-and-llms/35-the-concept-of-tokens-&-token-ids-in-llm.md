## Understanding Tokens in Large Language Models

The basic unit of a large language model is a **token**. Let's explore what tokens are and why they are essential in LLMs.

Previously, it was mentioned that the core job of an LLM is to guess the next word. However, that's not entirely accurate. LLMs actually predict the next **token**.

In simple terms, a token can be thought of as a word. However, the meaning of "token" is slightly different from the traditional definition of an English word.

### Why Tokens?

Large language models are artificial intelligence systems, and computers. They can't directly understand human languages like English. Computers only understand numbers. Therefore, everything we input into an LLM must be converted into numbers, which are **tokens**. üî¢

Similarly, LLMs produce tokens as output. These tokens (numbers) need to be converted back into normal English words. This conversion is handled by the LLM wrapper (e.g., ChatGPT).

*   Input: English words are converted into tokens.
*   Output: Tokens generated by the LLM are converted back into English words.

### What is a Token?

A token can be:

*   A character
*   A word
*   A part of a word

The value of a token depends on the specific LLM model.

üìå **Example:**

One LLM might treat "dog" as a single token, while another might treat it as two tokens: "d" and "og".

Most commonly used English words are typically treated as single tokens. Additionally, suffixes and prefixes like "-ish" and "-ing," as well as single characters (A, B, 1), and special keywords (e.g., end of text, end of prompt), are also treated as tokens.

üìù **Note:** Tokens are not exactly words; they are the pieces models use to build words.

### Tokenization Example: "playfully"

Consider the word "playfully." LLMs might split this word into multiple tokens:

1.  "play" (token one)
2.  "ful" (token two)
3.  "ly" (token three)

This allows LLMs to understand and generate even new or misspelled words by combining known tokens. This is why ChatGPT can often understand questions with spelling mistakes. üí° **Tip:** LLMs are flexible with new words, typos, or slang because they break them down into known tokens.

An LLM might already have a token for "play." Many English words use "ful" or "-ish" as suffixes or prefixes, so these will also have corresponding tokens. This is how the process works behind the scenes.

### Token IDs and Vectors

When you provide text to ChatGPT, each word or letter is converted into a token, and each token is assigned a unique **token ID**. Each of these token IDs has a specific meaning. From these token IDs, vector numbers are built by the LLM.

Don't worry, we'll discuss the concepts of token IDs and vectors in more detail soon.

### Demo: Tokenizer

Let's look at a demo using OpenAI's Platform Tokenizer to see how English words are converted into tokens.

If you type "play foolish" into the tokenizer, it's converted into three tokens: "play," "ful," and "ish." Each token is highlighted in a different color and has a unique token ID.

üìå **Example:**

For "play," the token ID is 2003. This token ID is static. If you type "play well," the token ID for "play" will still be 2003. However, if you capitalize the first letter ("Play well"), the token ID changes (e.g., to 10245).

Even though it seems trivial, computers need this distinction to differentiate between uppercase and lowercase letters.

üìå **Example:**

Typing "Happy for you" results in three tokens: "Happy," " for," and " you." The token ID for "Happy" is 37408.

If you type "Happy birthday," "birthday" is treated as a single token along with the space. The token ID for "Happy" remains 37408.

üìå **Example:**

Typing "You seems happy" results in different token IDs because " happy" (with a leading space) is considered a single token. The token ID for " happy" is 27213.

Adding a space can significantly change the token ID.

üìå **Example:**

Typing " Happy birthday" (with leading spaces) results in " Happy" and " birthday" as separate tokens.

If you remove the leading space, the token ID for "Happy" changes back to 37408.

Hopefully, you now have a clearer understanding of how tokens are formed from input text.

The next lecture will cover the meaning of these token IDs and how they are used inside LLM models.
