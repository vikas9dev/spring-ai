## How Large Language Models (LLMs) Work: Guessing the Next Word

At its core, a large language model (LLM) is designed to guess the next word in a sequence. This is the primary function an LLM performs behind the scenes. Think of it as a highly advanced autocomplete system.

Given an input sentence or word, the LLM predicts the next word based on the context.

üìå **Example:** If the input is "once upon a," the LLM might guess "time" to form the meaningful phrase "once upon a time." During training, the model has likely encountered similar sentences and learned the patterns.

### From Single Words to Full Responses

You might wonder: if LLMs only guess the next word, how do tools like ChatGPT provide full-length responses?

The answer is that applications like ChatGPT are **LM wrappers**. They are built by adding extra logic on top of the core LLM.

These wrappers make LLMs user-friendly by:

*   Generating full sentences.
*   Handling conversations.
*   Providing an interface for end-user interaction.

Analogy: Think of the LLM as an engine and the wrapper (like ChatGPT) as a car or vehicle.

### "Eating Its Own Output": Generating Full Responses

LM wrappers use a technique called "eating its own output" to generate complete responses from the underlying LLM. Here's how it works:

1.  **User Input:** You ask ChatGPT a question: "What is the capital of India?"
2.  **Wrapper Processing:** The wrapper (ChatGPT) modifies the question for the LLM. It might prefix the question with "user:" to indicate a user query and add a special character at the end to signal the end of the prompt.
3.  **LLM Prediction:** The LLM, based on its training, understands the question and generates the first word of the answer: "New Delhi."
4.  **Iterative Process:** The wrapper feeds "New Delhi" back to the LLM to generate the next word. This time, the LLM might predict "is."
5.  **Building the Sentence:** This process continues iteratively. The LLM receives the growing sentence ("New Delhi is") and predicts the next word ("the"), and so on.
6.  **End of Text Signal:** Eventually, the LLM generates the complete answer: "New Delhi is the capital of India." It also generates a special "end of text" token.
7.  **Wrapper Termination:** The wrapper recognizes the "end of text" token and stops feeding the input back to the LLM, concluding the response.

### LM Wrappers in the Industry

Many LM wrappers exist, including:

*   Google Gemini
*   ChatGPT
*   Claude (Anthropic)

These wrappers provide the conversational feel that users experience.

On the ChatGPT website, the dropdown menu allows you to select different underlying LM models. The ChatGPT wrapper then handles the interaction with the chosen model.

### The "Attention is All You Need" Paper

üí° **Tip:** The development of LLMs was significantly influenced by the white paper "Attention is All You Need," published in 2017 by Google researchers.

This paper introduced the **Transformer** architecture, a new neural network design.

*   The Transformer was initially used for machine translation tasks.
*   It demonstrated superior quality, parallelization, and training efficiency.

While Google published the paper, OpenAI was the first to release an LLM using the Transformer architecture with ChatGPT.

Initially, Google's LLM wrapper, Google Bard, was not as successful as ChatGPT. However, many vendors, including Microsoft and Twitter, began building LM wrappers. Google has since improved its offering with Google Gemini.

The "Attention is All You Need" paper can be accessed via a PDF link.

The paper details how the Transformer model works, using concepts like:

*   Embedding
*   Positional Encoding
*   Attention Layers

üìù **Note:** As a developer, you don't need to understand every detail of these models, but a basic understanding is beneficial. This series aims to provide that foundational knowledge.
